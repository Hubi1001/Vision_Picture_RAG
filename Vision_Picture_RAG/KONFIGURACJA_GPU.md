# ğŸš€ Konfiguracja GPU dla Vision RAG

## Status aktualny projektu

Projekt **czÄ™Å›ciowo** korzysta z GPU. Wprowadzone poprawki:

### âœ… Co juÅ¼ dziaÅ‚a:
1. **Model embeddingÃ³w (CLIP)** - automatycznie uÅ¼ywa GPU gdy dostÄ™pne
2. **Diagnostyka GPU** - pokazuje dostÄ™pnoÅ›Ä‡ i parametry
3. **Optymalizacja LLM** - dodano FP16 i device_map="auto"

### ğŸ”§ Co zostaÅ‚o poprawione:

#### 1. Model LLM (Phi-3)
**Przed:**
```python
llm_model.to(DEVICE)  # âŒ BÅÄ„D - brak przypisania
```

**Po:**
```python
if DEVICE == "cuda":
    llm_model = AutoModelForCausalLM.from_pretrained(
        LLM_MODEL, 
        torch_dtype=torch.float16,  # âœ… FP16 - 2x szybciej
        device_map="auto",           # âœ… Automatyczny podziaÅ‚ na GPU
        low_cpu_mem_usage=True
    )
else:
    llm_model = AutoModelForCausalLM.from_pretrained(LLM_MODEL)
    llm_model = llm_model.to(DEVICE)
```

#### 2. Diagnostyka GPU
Dodana komÃ³rka diagnostyczna w [metal_parts_rag.ipynb](metal_parts_rag.ipynb#L82-L115):
```python
print(f"âœ“ CUDA dostÄ™pne: {torch.cuda.is_available()}")
print(f"âœ“ GPU: {torch.cuda.get_device_name(0)}")
print(f"âœ“ PamiÄ™Ä‡ GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
```

#### 3. Test GPU
Nowa komÃ³rka testowa weryfikuje obliczenia na GPU:
- MnoÅ¼enie macierzy 1000x1000
- Pomiar czasu wykonania
- Sprawdzenie zajÄ™tej pamiÄ™ci

---

## ğŸ“‹ Instalacja PyTorch z CUDA

### Windows/Linux (GPU NVIDIA):
```bash
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

### MacOS (bez CUDA):
```bash
pip install torch torchvision
```

### Weryfikacja instalacji:
```python
import torch
print(f"CUDA: {torch.cuda.is_available()}")
print(f"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'Brak'}")
```

---

## ğŸ¯ Jak uruchomiÄ‡ projekt z GPU

### 1. SprawdÅº dostÄ™pnoÅ›Ä‡ GPU
Uruchom komÃ³rkÄ™ 1 w notebooku [metal_parts_rag.ipynb](metal_parts_rag.ipynb):
```
âœ“ CUDA dostÄ™pne: True
âœ“ GPU: NVIDIA GeForce RTX 3080
âœ“ PamiÄ™Ä‡ GPU: 10.0 GB
```

### 2. Uruchom test GPU
Wykonaj nowÄ… komÃ³rkÄ™ testowÄ…:
```
âœ“ Test obliczeniowy GPU: 2.45 ms
âœ“ Tensor na GPU: cuda:0
âœ… GPU dziaÅ‚a poprawnie!
```

### 3. ZaÅ‚aduj modele
Modele automatycznie wykorzystajÄ… GPU:
- **CLIP**: `embedding_model = SentenceTransformer(..., device='cuda')`
- **Phi-3**: `device_map="auto"` + `torch_dtype=torch.float16`

---

## âš¡ Optymalizacje GPU

### 1. Mixed Precision (FP16)
```python
# Automatyczne w Phi-3
torch_dtype=torch.float16  # 2x szybciej, 50% mniej pamiÄ™ci
```

### 2. Batch Processing
```python
# Przetwarzanie wielu obrazÃ³w naraz
embeddings = embedding_model.encode(images, batch_size=32)
```

### 3. Gradient Accumulation
```python
# Dla duÅ¼ych modeli przy maÅ‚ej pamiÄ™ci GPU
optimizer.zero_grad()
for batch in batches:
    loss = model(batch)
    loss.backward()
optimizer.step()
```

---

## ğŸ“Š Oczekiwane przyspieszenie

| Operacja | CPU (i7) | GPU (RTX 3080) | Przyspieszenie |
|----------|----------|----------------|----------------|
| CLIP embedding | 150 ms | 15 ms | **10x** |
| Phi-3 generacja | 8 s | 800 ms | **10x** |
| Batch 32 obrazÃ³w | 4.8 s | 300 ms | **16x** |

---

## ğŸ› Troubleshooting

### BÅ‚Ä…d: "CUDA out of memory"
```python
# Zmniejsz batch size
batch_size = 8  # zamiast 32

# WyczyÅ›Ä‡ cache GPU
torch.cuda.empty_cache()

# UÅ¼yj CPU dla duÅ¼ych modeli
DEVICE = "cpu"
```

### BÅ‚Ä…d: "No CUDA GPUs are available"
```bash
# SprawdÅº sterowniki NVIDIA
nvidia-smi

# Przeinstaluj PyTorch
pip uninstall torch torchvision
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

### Wolne dziaÅ‚anie mimo GPU
```python
# Upewnij siÄ™, Å¼e tensory sÄ… na GPU
inputs = inputs.to('cuda')  # âœ…
outputs = model(inputs)     # GPU

# BÅÄ„D: tensory na CPU
inputs = inputs  # âŒ domyÅ›lnie CPU
```

---

## ğŸ“ˆ Monitoring GPU

```python
# Podczas dziaÅ‚ania
import torch

print(f"ZajÄ™ta: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB")
print(f"Zarezerwowana: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB")
print(f"Maksymalna: {torch.cuda.max_memory_allocated(0) / 1024**3:.2f} GB")

# Reset statystyk
torch.cuda.reset_peak_memory_stats()
```

### nvidia-smi (Windows/Linux)
```bash
# Monitoring w czasie rzeczywistym
watch -n 1 nvidia-smi

# Pojedyncze sprawdzenie
nvidia-smi
```

---

## âœ… Checklist konfiguracji GPU

- [ ] Zainstalowany PyTorch z CUDA: `torch.cuda.is_available() == True`
- [ ] Uruchomiony test GPU: `komÃ³rka testowa pokazuje cuda:0`
- [ ] Model embeddingÃ³w na GPU: `embedding_model.device == 'cuda'`
- [ ] Model LLM z FP16: `llm_model.dtype == torch.float16`
- [ ] Tensory przenoszone na GPU: `.to(DEVICE)` w kodzie
- [ ] Brak bÅ‚Ä™dÃ³w CUDA OOM

---

## ğŸ“ Dodatkowe zasoby

- [PyTorch CUDA Semantics](https://pytorch.org/docs/stable/notes/cuda.html)
- [Hugging Face GPU Training](https://huggingface.co/docs/transformers/perf_train_gpu_one)
- [NVIDIA CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit)
